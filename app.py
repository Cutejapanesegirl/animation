import os
import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import re
import io
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# KoNLPy(OKT)ëŠ” ì„ íƒì  ì‚¬ìš©: ì„¤ì¹˜/ë¹Œë“œê°€ ë¶ˆê°€í•œ í™˜ê²½ì—ì„œëŠ” ì •ê·œì‹ ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´
try:
    from konlpy.tag import Okt  # type: ignore
    _okt: Okt | None = Okt()
except Exception:
    _okt = None

# --- ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="ì• ë‹ˆë©”ì´ì…˜ í”¼ë“œë°± ë¶„ì„ê¸°")

# --- ì•± ì œëª© ë° ì†Œê°œ ---
st.title("ğŸŒŸ ì• ë‹ˆë©”ì´ì…˜ í”¼ë“œë°± ë¶„ì„ê¸°")
st.markdown("ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ê°ì„± ë¶„í¬**ì™€ **ì£¼ìš” í‚¤ì›Œë“œ**ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.")
st.markdown("---")

# --- ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ë° í•„í„° ---
st.sidebar.header("íŒŒì¼ ì—…ë¡œë“œ ë° ì„¤ì •")
uploaded_file = st.sidebar.file_uploader(
    "CSV ë˜ëŠ” Excel íŒŒì¼ ì—…ë¡œë“œ", type=["csv", "xlsx"]
)
st.sidebar.markdown(
    "**ì°¸ê³ :** íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì§€ ì•Šìœ¼ë©´ ë¡œì»¬ì˜ '@feedback-data.csv' ë˜ëŠ” ì˜ˆì‹œ ë°ì´í„°ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤."
)

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_data
def load_data(file_uploader):
    """íŒŒì¼ ì—…ë¡œë”ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¡œì»¬ CSV ë˜ëŠ” ì˜ˆì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""                                     
    if file_uploader:
        try:
            if file_uploader.name.endswith('.csv'):
                df = pd.read_csv(file_uploader)
            else:
                df = pd.read_excel(file_uploader)
            return df
        except Exception as e:
            st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    else:
        # ë¡œì»¬ ê¸°ë³¸ CSV ê²½ë¡œ í™•ì¸
        default_path = os.path.join(os.getcwd(), "@feedback-data.csv")
        if os.path.exists(default_path):
            try:
                df = pd.read_csv(default_path)
                st.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ì–´ ë¡œì»¬ '@feedback-data.csv'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return df
            except Exception as e:
                st.warning(f"ë¡œì»¬ CSVë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
        example_data = {
            'date': pd.to_datetime(['2024-05-01', '2024-05-02', '2024-05-03', '2024-05-04', '2024-05-05',
                                    '2024-05-06', '2024-05-07', '2024-05-08', '2024-05-09', '2024-05-10',
                                    '2024-05-11', '2024-05-12']),
            'product': ['ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”', 'ë¡œë´‡ ì¹œêµ¬ ìš©ì´', 'ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”', 'ë¡œë´‡ ì¹œêµ¬ ìš©ì´', 'ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”',
                       'ìš°ì£¼ íƒí—˜ê°€ ë¦¬ì•„', 'ë¡œë´‡ ì¹œêµ¬ ìš©ì´', 'ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”', 'ìš°ì£¼ íƒí—˜ê°€ ë¦¬ì•„', 'ë¡œë´‡ ì¹œêµ¬ ìš©ì´',
                       'ìš°ì£¼ íƒí—˜ê°€ ë¦¬ì•„', 'ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”'],
            'rating': [5, 4, 1, 2, 5, 5, 3, 4, 1, 5, 4, 2],
            'feedback': [
                "ì •ë§ ì¬ë°Œê³  ê°ë™ì ì´ì—ˆì–´ìš”! ìºë¦­í„°ë„ ë„ˆë¬´ ê·€ì—½ê³  ê·¸ë¦¼ì²´ê°€ ì•„ë¦„ë‹¤ì›Œìš”.",
                "ìŠ¤í† ë¦¬ê°€ ì¢€ ëŠë¦° ê²ƒ ê°™ì§€ë§Œ, ë¡œë´‡ ë””ìì¸ì´ ì•„ì£¼ ë§ˆìŒì— ë“¤ì–´ìš”.",
                "ê²°ë§ì´ ë„ˆë¬´ í—ˆë¬´í•´ì„œ ì‹¤ë§í–ˆì–´ìš”. ë‹¤ìŒ ì‹œì¦Œì€ ê¸°ëŒ€í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒ ê°™ì•„ìš”.",
                "ì•¡ì…˜ ì¥ë©´ì´ í›Œë¥­í•˜ê³  ë°•ì§„ê° ë„˜ì³ì„œ ì¢‹ì•˜ì–´ìš”. ì‚¬ìš´ë“œ íš¨ê³¼ë„ ìµœê³ ì…ë‹ˆë‹¤.",
                "OSTê°€ ì •ë§ ì¢‹ì•„ì„œ ê³„ì† ë“£ê³  ì‹¶ì–´ìš”. ë‹¤ì‹œ ë´ë„ ë„ˆë¬´ ì¢‹ì•„ìš”. ê¼­ ë³´ì„¸ìš”.",
                "ìš°ì£¼ ë°°ê²½ê³¼ ìŠ¤í† ë¦¬ê°€ ì‹ ì„ í–ˆì–´ìš”. ëª°ì…ê°ì´ ìµœê³ ì…ë‹ˆë‹¤!",
                "ìºë¦­í„°ë“¤ ê°„ì˜ ëŒ€í™”ê°€ ì¡°ê¸ˆ ì–´ìƒ‰í•œ ëŠë‚Œì´ì—ìš”. ê·¸ë˜ë„ ë³¼ë§Œí•´ìš”.",
                "ì£¼ì¸ê³µì˜ ì„±ì¥ì´ ê¸°ëŒ€ë¼ìš”. ë‹¤ìŒ ì—í”¼ì†Œë“œê°€ ê¶ê¸ˆí•©ë‹ˆë‹¤.",
                "ë„ˆë¬´ ì–´ë ¤ì›Œì„œ ì´í•´í•˜ê¸° í˜ë“¤ì—ˆì–´ìš”. ì•„ì´ë“¤ì´ ë³´ê¸°ì—” ë¶€ì í•©í•œ ë“¯.",
                "ë¡œë´‡ë“¤ì´ ì‹¸ìš°ëŠ” ì¥ë©´ì´ ì •ë§ ë©‹ìˆì—ˆì–´ìš”. CGê°€ ì•„ì£¼ í›Œë¥­í•©ë‹ˆë‹¤.",
                "ë°˜ì „ì´ ì¸ìƒì ì´ì—ˆì–´ìš”! ë–¡ë°¥ íšŒìˆ˜ê°€ ê¹”ë”í•´ì„œ ì¢‹ì•˜ì–´ìš”.",
                "ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë»”í•˜ê³  ì§€ë£¨í•´ìš”. ìƒˆë¡œìš´ ì‹œë„ê°€ ì—†ì–´ì„œ ì•„ì‰½ë„¤ìš”."
            ]
        }
        df = pd.DataFrame(example_data)
        st.info("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•„ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return df

# --- í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ë“¤ ---
stopwords_korean = {
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ë‚´', 'ë‚˜', 'ë„', 'ë¶„', 'ë‹˜', 'ê³¼', 'ì˜', 'ì—', 'ì™€', 'ì€', 'ëŠ”', 'ë‹¤', 'ê³ ', 'ë©´', 'ë¡œ', 'ë¥¼', 'ê²Œ', 'ì˜í•´',
    'ì •ë§', 'ì§„ì§œ', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì•ˆ', 'ëª»', 'ë˜', 'ì˜', 'ì´ë ‡ë‹¤', 'ì €ë ‡ë‹¤', 'ê·¸ë ‡ë‹¤', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì•Šë‹¤'
}

def _tokenize_korean(text: str) -> list[str]:
    # OKTê°€ ìˆìœ¼ë©´ ëª…ì‚¬ ê¸°ì¤€, ì—†ìœ¼ë©´ í•œê¸€ 2ì ì´ìƒ í† í° ì •ê·œì‹ ì‚¬ìš©
    text = re.sub(r"[^ê°€-í£\s]", " ", text)
    if _okt is not None:
        try:
            return [n for n in _okt.nouns(text) if len(n) > 1]
        except Exception:
            pass
    return re.findall(r"[ê°€-í£]{2,}", text)

def analyze_sentiment(text):
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„"""
    positive_keywords = ['ì¬ë°Œ', 'ê°ë™', 'ì¢‹', 'ì•„ë¦„ë‹µ', 'ê·€ì—½', 'í›Œë¥­', 'ìµœê³ ', 'ë§ˆìŒì— ë“¤', 'ìµœê³ ', 'ì‚¬ë‘', 'ì¶”ì²œ']
    negative_keywords = ['ì‹¤ë§', 'ì•„ì‰½', 'í—ˆë¬´', 'ë³„ë¡œ', 'ì§€ë£¨', 'ëŠë¦¬', 'ë‹¨ì ']

    tokens = _tokenize_korean(text)
    
    pos_score = sum(1 for word in tokens if any(keyword in word for keyword in positive_keywords))
    neg_score = sum(1 for word in tokens if any(keyword in word for keyword in negative_keywords))
    
    if pos_score > neg_score:
        return 'ê¸ì •'
    elif neg_score > pos_score:
        return 'ë¶€ì •'
    else:
        return 'ì¤‘ë¦½'

def extract_keywords(text_series):
    """í”¼ë“œë°± í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    text_combined = " ".join(text_series.astype(str).tolist())
    nouns = _tokenize_korean(text_combined)
    filtered_nouns = [n for n in nouns if len(n) > 1 and n not in stopwords_korean]
    return Counter(filtered_nouns)

# --- ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í•¨ìˆ˜ë“¤ ---
def create_user_animation_matrix(df):
    """ì‚¬ìš©ì-ì• ë‹ˆë©”ì´ì…˜ í‰ì  ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
    # dateë¥¼ ì‚¬ìš©ì IDë¡œ ê°„ì£¼ (ì‹¤ì œë¡œëŠ” ë³„ë„ user_id ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨)
    user_ratings = df.groupby(['date', 'product'])['rating'].mean().reset_index()
    user_ratings.columns = ['user_id', 'product', 'rating']
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„±
    matrix = user_ratings.pivot_table(
        index='product', 
        columns='user_id', 
        values='rating', 
        aggfunc='mean'
    ).fillna(0)
    
    return matrix

def get_similar_animations(target_animation, df, top_n=3):
    """íŠ¹ì • ì• ë‹ˆë©”ì´ì…˜ê³¼ ìœ ì‚¬í•œ ì• ë‹ˆë©”ì´ì…˜ ì¶”ì²œ"""
    try:
        matrix = create_user_animation_matrix(df)
        
        if target_animation not in matrix.index:
            return []
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (item-based collaborative filtering)
        target_vector = matrix.loc[target_animation].values.reshape(1, -1)
        similarities = cosine_similarity(target_vector, matrix.values)[0]
        
        similarity_df = pd.DataFrame({
            'animation': matrix.index,
            'similarity': similarities
        })
        
        recommendations = similarity_df[
            similarity_df['animation'] != target_animation
        ].nlargest(top_n, 'similarity')
        
        return recommendations.to_dict('records')
    
    except Exception as e:
        st.warning(f"ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def get_user_based_recommendations(df, target_animations, top_n=3):
    """ì‚¬ìš©ìê°€ ë³¸ ì• ë‹ˆë©”ì´ì…˜ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ (ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§)"""
    try:
        matrix = create_user_animation_matrix(df)
        
        if matrix.empty or len(matrix.index) < 2:
            return []
        
        available_animations = [a for a in target_animations if a in matrix.index]
        if not available_animations:
            return []
        
        user_preferences = matrix.loc[available_animations].mean(axis=0)
        user_vector = user_preferences.values.reshape(1, -1)
        similarities = cosine_similarity(user_vector, matrix.values)[0]
        
        similarity_df = pd.DataFrame({
            'animation': matrix.index,
            'similarity': similarities
        })
        
        recommendations = similarity_df[
            ~similarity_df['animation'].isin(target_animations)
        ].nlargest(top_n, 'similarity')
        
        return recommendations.to_dict('records')
    
    except Exception as e:
        st.warning(f"ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- ë©”ì¸ ì•± ë¡œì§ ---
df = load_data(uploaded_file)

if df is not None:
    st.sidebar.subheader("ë°ì´í„° í•„í„°ë§")
    
    # ë‚ ì§œ í•„í„°
    df['date'] = pd.to_datetime(df['date'])
    min_date = df['date'].min()
    max_date = df['date'].max()
    date_range = st.sidebar.slider(
        "ê¸°ê°„ ì„ íƒ",
        min_value=min_date.date(),
        max_value=max_date.date(),
        value=(min_date.date(), max_date.date())
    )
    df_filtered = df[(df['date'].dt.date >= date_range[0]) & (df['date'].dt.date <= date_range[1])]

    # ì œí’ˆëª… í•„í„°
    if 'product' in df.columns:
        unique_products = df['product'].unique().tolist()
        selected_products = st.sidebar.multiselect(
            "ì• ë‹ˆë©”ì´ì…˜ ì„ íƒ",
            options=unique_products,
            default=unique_products
        )
        df_filtered = df_filtered[df_filtered['product'].isin(selected_products)]

    if st.button("ë¶„ì„ ì‹¤í–‰"):
        if df_filtered.empty:
            st.warning("ì„ íƒëœ í•„í„°ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner('í”¼ë“œë°±ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ğŸš€'):
                st.balloons()

                # ê°ì„± ë¶„ì„ ì‹¤í–‰
                df_filtered['sentiment'] = df_filtered['feedback'].apply(analyze_sentiment)

                # ì£¼ìš” ê²°ê³¼ ì‹œê°í™”
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ğŸ“Š ê°ì„± ë¶„í¬ ë¶„ì„")
                    sentiment_counts = df_filtered['sentiment'].value_counts().reset_index()
                    sentiment_counts.columns = ['sentiment', 'count']
                    fig_sentiment = px.pie(
                        sentiment_counts,
                        names='sentiment',
                        values='count',
                        title='ì „ì²´ í”¼ë“œë°± ê°ì„± ë¶„í¬',
                        color='sentiment',
                        color_discrete_map={'ê¸ì •': 'lightgreen', 'ì¤‘ë¦½': 'yellow', 'ë¶€ì •': 'salmon'}
                    )
                    st.plotly_chart(fig_sentiment, use_container_width=True)
                
                with col2:
                    st.subheader("ğŸ’¡ ì£¼ìš” í‚¤ì›Œë“œ")
                    keywords = extract_keywords(df_filtered['feedback'])
                    if keywords:
                        # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
                        # OSë³„ í°íŠ¸ ê²½ë¡œ íƒìƒ‰ (Windows/Streamlit Cloud Linux)
                        font_candidates = [
                            r"C:\\Windows\\Fonts\\malgun.ttf",  # Windows
                            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # Debian/Ubuntu
                            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",  # Noto CJK
                        ]
                        font_path = next((p for p in font_candidates if os.path.exists(p)), None)
                        wc = WordCloud(
                            font_path=font_path,
                            width=800,
                            height=400,
                            background_color='white'
                        ).generate_from_frequencies(keywords)
                        
                        fig, ax = plt.subplots(figsize=(10, 5))
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off")
                        st.pyplot(fig)
                        
                        # í‚¤ì›Œë“œ í…Œì´ë¸”
                        st.markdown("---")
                        st.markdown("##### ğŸ“Œ ìì£¼ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)")
                        top_keywords = keywords.most_common(10)
                        df_keywords = pd.DataFrame(top_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                        st.table(df_keywords)
                    else:
                        st.info("ë¶„ì„í•  í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                # ì¶”ì²œ ì„¹ì…˜ ì¶”ê°€
                st.markdown("---")
                st.subheader("ğŸ¯ ì• ë‹ˆë©”ì´ì…˜ ì¶”ì²œ")
                st.markdown("ë³¸ ì• ë‹ˆë©”ì´ì…˜ ê¸°ë°˜ìœ¼ë¡œ **ë¹„ìŠ·í•œ ì·¨í–¥ì˜ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•˜ëŠ”** ì• ë‹ˆë©”ì´ì…˜ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.")
                
                # ì¶”ì²œ ëª¨ë“œ ì„ íƒ
                recommendation_mode = st.radio(
                    "ì¶”ì²œ ë°©ì‹ ì„ íƒ",
                    ["ë³¸ ì• ë‹ˆë©”ì´ì…˜ë“¤ ê¸°ë°˜ ì¶”ì²œ", "íŠ¹ì • ì• ë‹ˆë©”ì´ì…˜ ìœ ì‚¬ ì¶”ì²œ"],
                    horizontal=True
                )
                
                if recommendation_mode == "ë³¸ ì• ë‹ˆë©”ì´ì…˜ë“¤ ê¸°ë°˜ ì¶”ì²œ":
                    # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì• ë‹ˆë©”ì´ì…˜ë“¤ ê¸°ë°˜ ì¶”ì²œ
                    if selected_products:
                        recommendations = get_user_based_recommendations(
                            df_filtered, 
                            selected_products, 
                            top_n=5
                        )
                        if recommendations:
                            st.success(f"âœ¨ **{', '.join(selected_products)}** ê¸°ë°˜ ì¶”ì²œ ê²°ê³¼")
                            for idx, rec in enumerate(recommendations, 1):
                                similarity_score = rec['similarity']
                                animation_name = rec['animation']
                                st.markdown(f"**{idx}. {animation_name}** (ìœ ì‚¬ë„: {similarity_score:.2%})")
                        else:
                            st.info("ì¶”ì²œí•  ì• ë‹ˆë©”ì´ì…˜ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    else:
                        st.info("ì¶”ì²œì„ ìœ„í•´ ìµœì†Œ 1ê°œ ì´ìƒì˜ ì• ë‹ˆë©”ì´ì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
                else:
                    # íŠ¹ì • ì• ë‹ˆë©”ì´ì…˜ ìœ ì‚¬ ì¶”ì²œ
                    target_animations_for_rec = st.selectbox(
                        "ê¸°ì¤€ ì• ë‹ˆë©”ì´ì…˜ ì„ íƒ",
                        options=unique_products,
                        help="ì´ ì• ë‹ˆë©”ì´ì…˜ê³¼ ìœ ì‚¬í•œ ì• ë‹ˆë©”ì´ì…˜ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤."
                    )
                    
                    if st.button("ìœ ì‚¬ ì• ë‹ˆë©”ì´ì…˜ ì°¾ê¸°"):
                        recommendations = get_similar_animations(
                            target_animations_for_rec,
                            df_filtered,
                            top_n=5
                        )
                        if recommendations:
                            st.success(f"âœ¨ **{target_animations_for_rec}**ì™€(ê³¼) ìœ ì‚¬í•œ ì• ë‹ˆë©”ì´ì…˜")
                            for idx, rec in enumerate(recommendations, 1):
                                similarity_score = rec['similarity']
                                animation_name = rec['animation']
                                st.markdown(f"**{idx}. {animation_name}** (ìœ ì‚¬ë„: {similarity_score:.2%})")
                        else:
                            st.info("ìœ ì‚¬í•œ ì• ë‹ˆë©”ì´ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        st.info("ë¶„ì„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

