import os
import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import re
import io

# KoNLPy(OKT)ëŠ” ì„ íƒì  ì‚¬ìš©: ì„¤ì¹˜/ë¹Œë“œê°€ ë¶ˆê°€í•œ í™˜ê²½ì—ì„œëŠ” ì •ê·œì‹ ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´
try:
    from konlpy.tag import Okt  # type: ignore
    _okt: Okt | None = Okt()
except Exception:
    _okt = None

# --- ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="ì• ë‹ˆë©”ì´ì…˜ í”¼ë“œë°± ë¶„ì„ê¸°")

# --- ì•± ì œëª© ë° ì†Œê°œ ---
st.title("ğŸŒŸ ì• ë‹ˆë©”ì´ì…˜ í”¼ë“œë°± ë¶„ì„ê¸°")
st.markdown("ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ê°ì„± ë¶„í¬**ì™€ **ì£¼ìš” í‚¤ì›Œë“œ**ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.")
st.markdown("---")

# --- ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ë° í•„í„° ---
st.sidebar.header("íŒŒì¼ ì—…ë¡œë“œ ë° ì„¤ì •")
uploaded_file = st.sidebar.file_uploader(
    "CSV ë˜ëŠ” Excel íŒŒì¼ ì—…ë¡œë“œ", type=["csv", "xlsx"]
)
st.sidebar.markdown(
    "**ì°¸ê³ :** íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì§€ ì•Šìœ¼ë©´ ë¡œì»¬ì˜ '@feedback-data.csv' ë˜ëŠ” ì˜ˆì‹œ ë°ì´í„°ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤."
)

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_data
def load_data(file_uploader):
    """íŒŒì¼ ì—…ë¡œë”ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¡œì»¬ CSV ë˜ëŠ” ì˜ˆì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""                                     
    if file_uploader:
        try:
            if file_uploader.name.endswith('.csv'):
                df = pd.read_csv(file_uploader)
            else:
                df = pd.read_excel(file_uploader)
            return df
        except Exception as e:
            st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    else:
        # ë¡œì»¬ ê¸°ë³¸ CSV ê²½ë¡œ í™•ì¸
        default_path = os.path.join(os.getcwd(), "@feedback-data.csv")
        if os.path.exists(default_path):
            try:
                df = pd.read_csv(default_path)
                st.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ì–´ ë¡œì»¬ '@feedback-data.csv'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return df
            except Exception as e:
                st.warning(f"ë¡œì»¬ CSVë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
        example_data = {
            'date': pd.to_datetime(['2024-05-01', '2024-05-02', '2024-05-03', '2024-05-04', '2024-05-05']),
            'product': ['ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”', 'ë¡œë´‡ ì¹œêµ¬ ìš©ì´', 'ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”', 'ë¡œë´‡ ì¹œêµ¬ ìš©ì´', 'ë§ˆë²•ì†Œë…€ ë§ˆë£¨ì½”'],
            'rating': [5, 4, 1, 2, 5],
            'feedback': [
                "ì •ë§ ì¬ë°Œê³  ê°ë™ì ì´ì—ˆì–´ìš”! ìºë¦­í„°ë„ ë„ˆë¬´ ê·€ì—½ê³  ê·¸ë¦¼ì²´ê°€ ì•„ë¦„ë‹¤ì›Œìš”.",
                "ìŠ¤í† ë¦¬ê°€ ì¢€ ëŠë¦° ê²ƒ ê°™ì§€ë§Œ, ë¡œë´‡ ë””ìì¸ì´ ì•„ì£¼ ë§ˆìŒì— ë“¤ì–´ìš”.",
                "ê²°ë§ì´ ë„ˆë¬´ í—ˆë¬´í•´ì„œ ì‹¤ë§í–ˆì–´ìš”. ë‹¤ìŒ ì‹œì¦Œì€ ê¸°ëŒ€í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒ ê°™ì•„ìš”.",
                "ì•¡ì…˜ ì¥ë©´ì´ í›Œë¥­í•˜ê³  ë°•ì§„ê° ë„˜ì³ì„œ ì¢‹ì•˜ì–´ìš”. ì‚¬ìš´ë“œ íš¨ê³¼ë„ ìµœê³ ì…ë‹ˆë‹¤.",
                "OSTê°€ ì •ë§ ì¢‹ì•„ì„œ ê³„ì† ë“£ê³  ì‹¶ì–´ìš”. ë‹¤ì‹œ ë´ë„ ë„ˆë¬´ ì¢‹ì•„ìš”. ê¼­ ë³´ì„¸ìš”."
            ]
        }
        df = pd.DataFrame(example_data)
        st.info("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•„ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return df

# --- í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ë“¤ ---
stopwords_korean = {
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ë‚´', 'ë‚˜', 'ë„', 'ë¶„', 'ë‹˜', 'ê³¼', 'ì˜', 'ì—', 'ì™€', 'ì€', 'ëŠ”', 'ë‹¤', 'ê³ ', 'ë©´', 'ë¡œ', 'ë¥¼', 'ê²Œ', 'ì˜í•´',
    'ì •ë§', 'ì§„ì§œ', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì•ˆ', 'ëª»', 'ë˜', 'ì˜', 'ì´ë ‡ë‹¤', 'ì €ë ‡ë‹¤', 'ê·¸ë ‡ë‹¤', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì•Šë‹¤'
}

def _tokenize_korean(text: str) -> list[str]:
    # OKTê°€ ìˆìœ¼ë©´ ëª…ì‚¬ ê¸°ì¤€, ì—†ìœ¼ë©´ í•œê¸€ 2ì ì´ìƒ í† í° ì •ê·œì‹ ì‚¬ìš©
    text = re.sub(r"[^ê°€-í£\s]", " ", text)
    if _okt is not None:
        try:
            return [n for n in _okt.nouns(text) if len(n) > 1]
        except Exception:
            pass
    return re.findall(r"[ê°€-í£]{2,}", text)

def analyze_sentiment(text):
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„"""
    positive_keywords = ['ì¬ë°Œ', 'ê°ë™', 'ì¢‹', 'ì•„ë¦„ë‹µ', 'ê·€ì—½', 'í›Œë¥­', 'ìµœê³ ', 'ë§ˆìŒì— ë“¤', 'ìµœê³ ', 'ì‚¬ë‘', 'ì¶”ì²œ']
    negative_keywords = ['ì‹¤ë§', 'ì•„ì‰½', 'í—ˆë¬´', 'ë³„ë¡œ', 'ì§€ë£¨', 'ëŠë¦¬', 'ë‹¨ì ']

    tokens = _tokenize_korean(text)
    
    pos_score = sum(1 for word in tokens if any(keyword in word for keyword in positive_keywords))
    neg_score = sum(1 for word in tokens if any(keyword in word for keyword in negative_keywords))
    
    if pos_score > neg_score:
        return 'ê¸ì •'
    elif neg_score > pos_score:
        return 'ë¶€ì •'
    else:
        return 'ì¤‘ë¦½'

def extract_keywords(text_series):
    """í”¼ë“œë°± í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    text_combined = " ".join(text_series.astype(str).tolist())
    nouns = _tokenize_korean(text_combined)
    filtered_nouns = [n for n in nouns if len(n) > 1 and n not in stopwords_korean]
    return Counter(filtered_nouns)

# --- ë©”ì¸ ì•± ë¡œì§ ---
df = load_data(uploaded_file)

if df is not None:
    st.sidebar.subheader("ë°ì´í„° í•„í„°ë§")
    
    # ë‚ ì§œ í•„í„°
    df['date'] = pd.to_datetime(df['date'])
    min_date = df['date'].min()
    max_date = df['date'].max()
    date_range = st.sidebar.slider(
        "ê¸°ê°„ ì„ íƒ",
        min_value=min_date.date(),
        max_value=max_date.date(),
        value=(min_date.date(), max_date.date())
    )
    df_filtered = df[(df['date'].dt.date >= date_range[0]) & (df['date'].dt.date <= date_range[1])]

    # ì œí’ˆëª… í•„í„°
    if 'product' in df.columns:
        unique_products = df['product'].unique().tolist()
        selected_products = st.sidebar.multiselect(
            "ì• ë‹ˆë©”ì´ì…˜ ì„ íƒ",
            options=unique_products,
            default=unique_products
        )
        df_filtered = df_filtered[df_filtered['product'].isin(selected_products)]

    if st.button("ë¶„ì„ ì‹¤í–‰"):
        if df_filtered.empty:
            st.warning("ì„ íƒëœ í•„í„°ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner('í”¼ë“œë°±ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ğŸš€'):
                st.balloons()

                # ê°ì„± ë¶„ì„ ì‹¤í–‰
                df_filtered['sentiment'] = df_filtered['feedback'].apply(analyze_sentiment)

                # ì£¼ìš” ê²°ê³¼ ì‹œê°í™”
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ğŸ“Š ê°ì„± ë¶„í¬ ë¶„ì„")
                    sentiment_counts = df_filtered['sentiment'].value_counts().reset_index()
                    sentiment_counts.columns = ['sentiment', 'count']
                    fig_sentiment = px.pie(
                        sentiment_counts,
                        names='sentiment',
                        values='count',
                        title='ì „ì²´ í”¼ë“œë°± ê°ì„± ë¶„í¬',
                        color='sentiment',
                        color_discrete_map={'ê¸ì •': 'lightgreen', 'ì¤‘ë¦½': 'yellow', 'ë¶€ì •': 'salmon'}
                    )
                    st.plotly_chart(fig_sentiment, use_container_width=True)
                
                with col2:
                    st.subheader("ğŸ’¡ ì£¼ìš” í‚¤ì›Œë“œ")
                    keywords = extract_keywords(df_filtered['feedback'])
                    if keywords:
                        # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
                        # OSë³„ í°íŠ¸ ê²½ë¡œ íƒìƒ‰ (Windows/Streamlit Cloud Linux)
                        font_candidates = [
                            r"C:\\Windows\\Fonts\\malgun.ttf",  # Windows
                            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # Debian/Ubuntu
                            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",  # Noto CJK
                        ]
                        font_path = next((p for p in font_candidates if os.path.exists(p)), None)
                        wc = WordCloud(
                            font_path=font_path,
                            width=800,
                            height=400,
                            background_color='white'
                        ).generate_from_frequencies(keywords)
                        
                        fig, ax = plt.subplots(figsize=(10, 5))
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off")
                        st.pyplot(fig)
                        
                        # í‚¤ì›Œë“œ í…Œì´ë¸”
                        st.markdown("---")
                        st.markdown("##### ğŸ“Œ ìì£¼ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)")
                        top_keywords = keywords.most_common(10)
                        df_keywords = pd.DataFrame(top_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                        st.table(df_keywords)
                    else:
                        st.info("ë¶„ì„í•  í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        st.info("ë¶„ì„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

